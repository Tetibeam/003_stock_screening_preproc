{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd607b5",
   "metadata": {},
   "source": [
    "## 初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Notebook初期設定 ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import src.config as cfg\n",
    "import src.data_loader as dl\n",
    "import src.cleaning_utils as cu\n",
    "print(\"🔁 autoreload 有効化完了\")\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79cbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定読み込み\n",
    "settings = cfg.load_settings(\"setting.yaml\")\n",
    "display(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6ff0d",
   "metadata": {},
   "source": [
    "## 本ファイルの説明\n",
    "データが最低限の品質基準を満たしているかを網羅的に確認します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdaa036",
   "metadata": {},
   "source": [
    "### データのロードの確認\n",
    "まずは欠損値の表現をチェックします。  \n",
    "その後、各ファイルの企業コード数、年度数、列数をチェックします。\n",
    "\n",
    "<details>\n",
    "<summary><b>結果</b></summary>\n",
    "\n",
    "2025年の企業コード数と年度数が多くなっています。  \n",
    "2025年には2023年と2024年の更新データが含まれていると推測されます。  \n",
    "そのため、2023年と2024年のデータを最新に更新し、2025年のデータから抜き取ります\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cdb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ファイル毎にデータを取得します\n",
      "\n",
      "ファイルごとに各文字列の個数をカウントします\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>nan</th>\n",
       "      <th>na</th>\n",
       "      <th>n/a</th>\n",
       "      <th>-</th>\n",
       "      <th>--</th>\n",
       "      <th>none</th>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>alphabet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [col, nan, na, n/a, -, --, none, 0,  , , alphabet]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#display(ser_str)\u001b[39;00m\n\u001b[32m     23\u001b[39m counts.append(\n\u001b[32m     24\u001b[39m     (col,\n\u001b[32m     25\u001b[39m     (ser_str == \u001b[33m'\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m'\u001b[39m).sum(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     ser_str.str.contains(\u001b[33m'\u001b[39m\u001b[33m[A-Za-z]\u001b[39m\u001b[33m'\u001b[39m, na=\u001b[38;5;28;01mFalse\u001b[39;00m).sum())\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mdf_placeholder_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_placeholder_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m = counts\n\u001b[32m     37\u001b[39m display(df_placeholder_counts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\cases\\portfolio\\003_stock_screening_preproc\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:912\u001b[39m, in \u001b[36m_LocationIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;28mself\u001b[39m._has_valid_setitem_indexer(key)\n\u001b[32m    911\u001b[39m iloc = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name == \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.iloc\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[43miloc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\cases\\portfolio\\003_stock_screening_preproc\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1933\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer\u001b[39m\u001b[34m(self, indexer, value, name)\u001b[39m\n\u001b[32m   1930\u001b[39m     indexer, missing = convert_missing_indexer(indexer)\n\u001b[32m   1932\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[32m-> \u001b[39m\u001b[32m1933\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1934\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1936\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mloc\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1937\u001b[39m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\cases\\portfolio\\003_stock_screening_preproc\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2307\u001b[39m, in \u001b[36m_iLocIndexer._setitem_with_indexer_missing\u001b[39m\u001b[34m(self, indexer, value)\u001b[39m\n\u001b[32m   2304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[32m   2305\u001b[39m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[32m   2306\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj.columns):\n\u001b[32m-> \u001b[39m\u001b[32m2307\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot set a row with mismatched columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2309\u001b[39m     value = Series(value, index=\u001b[38;5;28mself\u001b[39m.obj.columns, name=indexer)\n\u001b[32m   2311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj):\n\u001b[32m   2312\u001b[39m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[32m   2313\u001b[39m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "# 欠損値の表現確認\n",
    "print(\"\")\n",
    "print(\"ファイル毎にデータを取得します\")\n",
    "df_DATAs_BY_ALL_FILEs = dl.load_data_by_files(\n",
    "    settings[\"data_path\"][\"raw\"],settings[\"years\"],settings[\"files\"][\"raw\"],[\"\"]\n",
    ")\n",
    "#display(df_DATAs_BY_ALL_FILEs)\n",
    "\n",
    "print(\"\")\n",
    "print(\"ファイルごとに各文字列の個数をカウントします\")\n",
    "#\"nan\", \"na\", \"n/a\", \"null\", \"-\", \"--\", \"none\", \"0\",\" \",\"\"\n",
    "\n",
    "df_placeholder_counts = pd.DataFrame(\n",
    "    columns=[\"col\",\"nan\",\"na\",\"n/a\",\"-\",\"--\",\"none\",\"0\",\"space\",\"\",\"alphabet\"],\n",
    ")\n",
    "display(df_placeholder_counts)\n",
    "for (filename,year), df in df_DATAs_BY_ALL_FILEs.items():\n",
    "    counts = []\n",
    "    for col in df:\n",
    "        ser = df[col].fillna(\"\")\n",
    "        ser_str = ser.astype(str)\n",
    "        #display(ser_str)\n",
    "        counts.append(\n",
    "            (col,\n",
    "            (ser_str == 'nan').sum(),\n",
    "            (ser_str == 'na').sum(),\n",
    "            (ser_str == 'n/a').sum(),\n",
    "            (ser_str == '-').sum(),\n",
    "            (ser_str == '--').sum(),\n",
    "            (ser_str == 'none').sum(),\n",
    "            (ser_str == '0').sum(),\n",
    "            (ser_str == ' ').sum(),\n",
    "            (ser_str == '').sum(),\n",
    "            ser_str.str.contains('[A-Za-z]', na=False).sum())\n",
    "        )\n",
    "        df_placeholder_counts.loc[df_placeholder_counts.index.max()+1] = counts\n",
    "        display(df_placeholder_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 財務データ 結合前\n",
    "pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_rows', 60)\n",
    "\n",
    "def get_file_info(df_datas_by_all_files):\n",
    "    df_file_info = pd.DataFrame(columns=[\"file\",\"year\",\"code_counts\",\"year_counts\",\"column_counts\"])\n",
    "    for file, year in df_datas_by_all_files.keys():\n",
    "        df = df_datas_by_all_files[(file,year)]\n",
    "        # len(df_file_info)を次の新しい行のインデックスとして明示的に指定します。\n",
    "        df_file_info.loc[len(df_file_info)] = [\n",
    "            file, year, df[\"コード\"].nunique(), df[\"年度\"].nunique(),df.columns.nunique()\n",
    "        ]\n",
    "    return df_file_info\n",
    "\n",
    "print(\"\")\n",
    "print(\"ファイル毎にデータを取得します\")\n",
    "df_DATAs_BY_ALL_FILEs = dl.load_data_by_files(\n",
    "    settings[\"data_path\"][\"raw\"],settings[\"years\"],settings[\"files\"][\"raw\"],[\"-\",\"\",\"0\"]\n",
    ")\n",
    "#display(df_DATAs_BY_ALL_FILEs)\n",
    "\n",
    "print(\"\")\n",
    "print(\"企業コード数、エンドの数、列数をファイルごとにまとめます。\")\n",
    "df_file_info = get_file_info(df_DATAs_BY_ALL_FILEs)\n",
    "#display(df_file_info)\n",
    "\n",
    "print(\"\")\n",
    "print(\"各ファイルの年推移を可視化します。\")\n",
    "\"\"\"fig = px.line(df_file_info,x=\"year\",y=\"code_counts\",color=\"file\")\n",
    "fig.show()\n",
    "fig = px.line(df_file_info,x=\"year\",y=\"year_counts\",color=\"file\")\n",
    "fig.show()\n",
    "fig = px.line(df_file_info,x=\"year\",y=\"column_counts\",color=\"file\")\n",
    "fig.show()\"\"\"\n",
    "\n",
    "print(\"\")\n",
    "print(\"列数はOKです\")\n",
    "print(\"最新年のファイルに登録されている年度を表示します。\")\n",
    "df = df_DATAs_BY_ALL_FILEs[(settings[\"files\"][\"raw\"][0],2025)]\n",
    "#display(df[\"年度\"].unique())\n",
    "\n",
    "print(\"\")\n",
    "print(\"最新年のファイルに過去のデータがあります。\")\n",
    "print(\"最新年に登録されている過去の年度と過去のデータが同じか確認します\")\n",
    "print(\"まずは、年度を限定して、コードの一覧を見ます\")\n",
    "#display(df[df[\"年度\"]==\"2024/12\"])\n",
    "\n",
    "print(\"\")\n",
    "print(\"コードを限定し同じかどうか調べます。\")\n",
    "df = df_DATAs_BY_ALL_FILEs[(settings[\"files\"][\"raw\"][0],2025)]\n",
    "#display(df[df[\"コード\"]==\"130A\"])\n",
    "df = df_DATAs_BY_ALL_FILEs[(settings[\"files\"][\"raw\"][0],2024)]\n",
    "#display(df[df[\"コード\"]==\"130A\"])\n",
    "\n",
    "print(\"\")\n",
    "print(\"違っています。これは更新データと思われます。\")\n",
    "print(\"最新年度にある過去のデータを取得し、過去のデータを更新します\")\n",
    "df_DATAs_BY_ALL_FILEs = dl.update_duplicated(df_DATAs_BY_ALL_FILEs, 2025)\n",
    "#display(df_DATAs_BY_ALL_FILEs)\n",
    "\n",
    "print(\"\")\n",
    "print(\"正しく処理が行われ、最新年に登録されている過去の年度と過去のデータが同じか確認します。\")\n",
    "df = df_DATAs_BY_ALL_FILEs[(settings[\"files\"][\"raw\"][0],2025)]\n",
    "#display(df[df[\"コード\"]==\"130A\"])\n",
    "df = df_DATAs_BY_ALL_FILEs[(settings[\"files\"][\"raw\"][0],2024)]\n",
    "#display(df[df[\"コード\"]==\"130A\"])\n",
    "\n",
    "print(\"\")\n",
    "print(\"最後に最新年度にある過去のデータを消去します。\")\n",
    "cutoff_date = pd.to_datetime(\"2025-01-01\")\n",
    "for file, year in df_DATAs_BY_ALL_FILEs.keys():\n",
    "    if year == 2025:\n",
    "        df = df_DATAs_BY_ALL_FILEs[(file,year)]\n",
    "        df = df[df[\"年度\"] >= cutoff_date]\n",
    "        df_DATAs_BY_ALL_FILEs[(file,year)] = df\n",
    "print(\"\")\n",
    "print(\"各ファイルの年推移をもう一度可視化し、改善していることを確かめます。\") \n",
    "df_file_info_after = get_file_info(df_DATAs_BY_ALL_FILEs)\n",
    "df_file_info_after[\"区分\"] = \"処理後\"\n",
    "df_file_info_after = df_file_info_after.set_index([\"file\", \"year\"]).sort_index()\n",
    "df_file_info[\"区分\"] = \"処理前\"\n",
    "df_file_info = df_file_info.set_index([\"file\", \"year\"]).sort_index()\n",
    "df_compare = pd.concat([df_file_info,df_file_info_after]).sort_index().reset_index()\n",
    "#display(df_compare)\n",
    "fig = px.line(df_compare,x=\"year\",y=\"code_counts\",color=\"区分\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 財務データ ファイルごとの結合\n",
    "df_DATAs_BY_FILEs = dl.load_yearly_data(settings[\"data_path\"], settings[\"years\"], settings[\"files\"],)\n",
    "target_file = settings[\"files\"][0]\n",
    "display(df_DATAs_BY_FILEs[target_file][df_DATAs_BY_FILEs[target_file][\"コード\"]==\"130A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 財務データ　全結合\n",
    "df_ALL_DATAs = dl.merge_all_data(df_DATAs_BY_FILEs)\n",
    "print(df_ALL_DATAs.shape)\n",
    "#display(df_ALL_DATAs)\n",
    "display(df_ALL_DATAs[\"コード\"].unique())\n",
    "display(df_ALL_DATAs[\"年度\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6804e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上場企業の情報\n",
    "df = dl.load_code_list_info(settings[\"data_path\"], settings[\"files_reference\"][0])\n",
    "print(settings[\"files_reference\"][0], df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeca61",
   "metadata": {},
   "source": [
    "### データ型の整合性チェック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc595948",
   "metadata": {},
   "source": [
    "### 初期品質の概要把握"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
