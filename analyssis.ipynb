{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š åˆ†æã®æµã‚Œ\n",
    "<details>\n",
    "<summary><b>ç›®æ¬¡</b></summary>\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿**\n",
    "1. **æ¬ æå€¤å‡¦ç†**\n",
    "    - æ¬ æåˆ—å‡¦ç†  \n",
    "    - æ¬ æå€¤å‡¦ç†  \n",
    "    - ç•°å¸¸å€¤å‡¦ç†  \n",
    "1. **é›†è¨ˆ**\n",
    "1. **ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›**\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šèª­ã¿è¾¼ã¿\n",
    "with open(\"setting.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    setting = yaml.safe_load(f)\n",
    "base_path = setting[\"data_path\"]\n",
    "years = setting[\"years\"]\n",
    "files = setting[\"files\"]\n",
    "output_path = setting[\"output\"][\"base_path\"]\n",
    "output_files = setting[\"output\"][\"files\"]\n",
    "# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” æ¬ æåˆ—å‡¦ç†\n",
    "4ã¤ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—ãŒå„å¹´åº¦ã§åŒã˜ã‹ã‚’ç›®è¦–ã§ç¢ºèªã—ã¦ãŠãã¾ã™ã€‚\n",
    "å•é¡ŒãŒã‚ã‚‹å ´åˆã¯å‡¦ç†ã—ã¾ã™ã€‚\n",
    "<details><summary><b>çµæœ</b></summary>\n",
    "ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§åŒã˜åˆ—ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "\n",
    "- fy-balance-sheet.csv\n",
    "- fy-cash-flow-statement.csv\n",
    "- fy-profit-and-loss.csv\n",
    "- fy-stock-dividend.csv\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in files:\n",
    "    merged = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(base_path, str(year), filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path,header=1)\n",
    "            print(\"å¹´ï¼š\",year,\" \",\"ãƒ•ã‚¡ã‚¤ãƒ«å:\",filename,\" \",\"åˆ—\",df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ æ¬ æå€¤å‡¦ç†\n",
    "ã“ã“ã§ã¯ã€å¾Œã®åˆ†æãƒ»ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®å‡¦ç†ã‚’é©åˆ‡ã«è¨­è¨ˆã™ã‚‹ãŸã‚ã€æ¬ æå€¤ã®å‚¾å‘ã‚’ç¢ºèªã—ã¾ã™ã€‚  \n",
    "âš ï¸ **æ³¨æ„â€»** csvãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æ¬ æå€¤ã«\"-\"ãŒå…¥ã£ã¦ã„ã‚‹ãŸã‚ã€NaNã«ç½®ãæ›ãˆã¦èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "<details><summary><b>åˆ†æå†…å®¹</b></summary>\n",
    "\n",
    "- åˆ—ã”ã¨ã®æ¬ æå€¤ã®æ•°ãƒ»å‰²åˆã‚’ç¢ºèªã™ã‚‹\n",
    "- å¹´ã”ã¨ã®æ¬ æç‡ã®å‚¾å‘ã‚’ç¢ºèª\n",
    "- å…¨ä¼æ¥­ã®æ¬ æç‡åˆ†å¸ƒã‚’è¦‹ã‚‹\n",
    "<br>\n",
    "</details>\n",
    "<details><summary><b>çµæœã¨è€ƒå¯Ÿ</b></summary>\n",
    "\n",
    "- fy-balance-sheet.csv\n",
    "- fy-cash-flow-statement.csv\n",
    "- fy-profit-and-loss.csv\n",
    "- fy-stock-dividend.csv\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ï¼”ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«çµåˆ\n",
    "all_df = {}\n",
    "missing_df = {}\n",
    "for filename in files:\n",
    "    if filename not in missing_df:\n",
    "        all_df[filename] = pd.DataFrame()\n",
    "        missing_df[filename] = pd.DataFrame()\n",
    "    for year in years:\n",
    "        file_path = os.path.join(base_path, str(year), filename)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path,header=1,na_values=\"-\")\n",
    "            all_df[filename] = pd.concat([all_df[filename], df], ignore_index=True) # å…¨ãƒ‡ãƒ¼ã‚¿çµåˆ\n",
    "            missing_rate = df.isnull().mean().to_frame(name=\"missing_rate\")\n",
    "            missing_rate[\"year\"] = year\n",
    "            missing_rate = missing_rate.reset_index().rename(columns={\"index\": \"column_name\"})\n",
    "            missing_df[filename] = pd.concat([missing_df[filename], missing_rate], ignore_index=True)\n",
    "#display(all_df[files[0]].head(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ—ã”ã¨ã®æ¬ æå€¤ã®æ•°ãƒ»å‰²åˆã‚’ç¢ºèªã™ã‚‹\n",
    "for filename in files:\n",
    "    print(\"ãƒ•ã‚¡ã‚¤ãƒ«å:\",filename)\n",
    "    #display(all_df[filename].shape)\n",
    "    #display(all_df[filename].isna().sum())\n",
    "    display(all_df[filename].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¹´ã”ã¨ã®æ¬ æç‡ã®å‚¾å‘ã‚’ç¢ºèªï¼ˆæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼‰\n",
    "\n",
    "# å„ã‚«ãƒ©ãƒ ã®æ¬ æç‡æ¨ç§»ã‚’æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§è¡¨ç¤º\n",
    "for filename in files:\n",
    "    fig = px.line(\n",
    "        missing_df[filename],  # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "        x=\"year\",\n",
    "        y=\"missing_rate\",\n",
    "        color=\"column_name\",   # åˆ—ã”ã¨ã«è‰²åˆ†ã‘\n",
    "        markers=True,\n",
    "        title=\"å„åˆ—ã®æ¬ æç‡ã®æ¨ç§»\"\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¹´ã”ã¨ã®æ¬ æç‡ã®å‚¾å‘ã‚’ç¢ºèªï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‰\n",
    "\n",
    "for filename in missing_df:\n",
    "    pivot_table = missing_df[filename].pivot(index=\"column_name\", columns=\"year\", values=\"missing_rate\")\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_table.values,\n",
    "        x=pivot_table.columns,\n",
    "        y=pivot_table.index,\n",
    "        colorscale=\"Viridis\",\n",
    "        colorbar=dict(title=\"Missing Rate\")\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"{filename} ã®æ¬ æç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\",\n",
    "        xaxis_title=\"Year\",\n",
    "        yaxis_title=\"Column Name\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ä¼æ¥­ã®æ¬ æç‡åˆ†å¸ƒã‚’è¦‹ã‚‹ï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰\n",
    "missing_score = {}\n",
    "for filename in files:\n",
    "    # ã‚³ãƒ¼ãƒ‰ã”ã¨ã«æ¬ æç‡ã‚’è¨ˆç®—\n",
    "    missing_score[filename] = pd.DataFrame()\n",
    "    missing_score[filename] = (\n",
    "        all_df[filename]\n",
    "        .groupby(\"ã‚³ãƒ¼ãƒ‰\")\n",
    "        .agg(lambda x: x.isnull().mean())  # å„åˆ—ã”ã¨ã®æ¬ æç‡\n",
    "        .mean(axis=1)                      # åˆ—å¹³å‡\n",
    "        .reset_index(name=\"å¹³å‡æ¬ æç‡\")\n",
    "    )\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #display(missing_score[filename])\n",
    "# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤º\n",
    "fig = px.histogram(\n",
    "    missing_score[files[0]],  # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "    x=\"å¹³å‡æ¬ æç‡\",\n",
    "    nbins=50,\n",
    "    title=\"å…¨ä¼æ¥­ã®æ¬ æç‡åˆ†å¸ƒ\"+filename,\n",
    "    labels={\"å¹³å‡æ¬ æç‡\": \"å¹³å‡æ¬ æç‡\"}\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_rate = 0.22  # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã§çªå‡ºã—ã¦ã„ã‚‹æ¬ æç‡\n",
    "tolerance = 0.005\n",
    "\n",
    "peak_group = missing_score[filename][\n",
    "    (missing_score[filename][\"å¹³å‡æ¬ æç‡\"] > peak_rate - tolerance) &\n",
    "    (missing_score[filename][\"å¹³å‡æ¬ æç‡\"] < peak_rate + tolerance)\n",
    "]\n",
    "\n",
    "print(peak_group.sort_values(by=\"å¹³å‡æ¬ æç‡\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD --- IGNORE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR bankã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å¸ã„ä¸Šã’ã‚‹\n",
    "import pandas as pd\n",
    "import myLibStandard as myLib\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "# BSã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "dfBalanceSheet = pd.DataFrame()\n",
    "dl = [\"fy-balance-sheet\", \"fy-cash-flow-statement\", \"fy-profit-and-loss\", \"fy-stock-dividend\"]\n",
    "for folder in range(2010, 2026):\n",
    "    df = pd.read_csv(myLib.pathDataDomesticData + str(folder) + \"/fy-balance-sheet.csv\",\\\n",
    "    header=1, index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "    dfBalanceSheet = pd.concat([dfBalanceSheet, df], axis=0, join=\"outer\")\n",
    "dfBalanceSheet = dfBalanceSheet.sort_index()\n",
    "dfBalanceSheet = dfBalanceSheet.loc[~dfBalanceSheet.index.duplicated(\"last\")]\n",
    "\n",
    "# PLã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "dfProftLoss = pd.DataFrame()\n",
    "for folder in range(2010, 2026):\n",
    "    df = pd.read_csv(myLib.pathDataDomesticData + str(folder) + \"/\" + \"fy-profit-and-loss.csv\",\\\n",
    "    header=1, index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "    dfProftLoss = pd.concat([dfProftLoss, df], axis=0, join=\"outer\")\n",
    "dfProftLoss = dfProftLoss.sort_index()\n",
    "dfProftLoss = dfProftLoss.loc[~dfProftLoss.index.duplicated(\"last\")]\n",
    "\n",
    "# SDã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "dfStockDividend = pd.DataFrame()\n",
    "for folder in range(2010, 2026):\n",
    "    df = pd.read_csv(myLib.pathDataDomesticData + str(folder) + \"/\" + \"fy-stock-dividend.csv\",\\\n",
    "    header=1, index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "    dfStockDividend = pd.concat([dfStockDividend, df], axis=0, join=\"outer\")\n",
    "dfStockDividend = dfStockDividend.loc[~dfStockDividend.index.duplicated(\"last\")]\n",
    "dfStockDividend = dfStockDividend.sort_index(level=[\"ã‚³ãƒ¼ãƒ‰\"])\n",
    "#display(dfStockDividend)\n",
    "\n",
    "dfAll = pd.concat([dfBalanceSheet, dfProftLoss], axis=1, join=\"outer\")\n",
    "dfAll = pd.concat([dfAll, dfStockDividend], axis=1, join=\"outer\")\n",
    "dfAll = dfAll.sort_index()\n",
    "#display(dfAll.head(1000))\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã‚»ãƒ«ã‚’\"-\"ã‹ã‚‰nullã«å¤‰æ›ã™ã‚‹\n",
    "import pandas as pd\n",
    "import math\n",
    "import myLibStandard as myLib\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\",index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype=\"str\")\n",
    "dfAll = dfAll.sort_index()\n",
    "for column in dfAll:\n",
    "    dfAll[column] = dfAll[column].replace(\"-\", math.nan)\n",
    "    dfAll[column] = dfAll[column].replace(0, math.nan)\n",
    "\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãŒ0ã®å ´åˆã¯NANã«ã™ã‚‹ï¼ˆã‚¼ãƒ­å‰²ã‚Šç®—ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰\n",
    "import pandas as pd\n",
    "import math\n",
    "import myLibStandard as myLib\n",
    "myLib.set_DisplayOption()\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\", dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "\n",
    "for i in dfAll.index:\n",
    "    if dfAll.loc[i, \"EPS\"] == 0:\n",
    "        dfAll.loc[i, \"EPS\"] = math.nan\n",
    "    if dfAll.loc[i, \"BPS\"] == 0:\n",
    "        dfAll.loc[i, \"BPS\"] = math.nan\n",
    "dfAll = dfAll.set_index([\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"])\n",
    "#display(dfAll.head(1000))\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### éŠ˜æŸ„åã‚„æ¥­ç¨®ã‚’å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆå–å¾—ã€ä¿å­˜\n",
    "from stocksymbol import StockSymbol\n",
    "api_key = '63ed951b-b643-48b1-bfe7-90c7a4f96349'\n",
    "ss = StockSymbol(api_key)\n",
    "symbol_list_japan = ss.get_symbol_list(market=\"Japan\") # \"us\" or \"america\" will also work\n",
    "df = symbol_list_japan\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(myLib.pathDataSummaryDomesticData + \"symbol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeä¸€è¦§ã‹ã‚‰éŠ˜æŸ„ã®æƒ…å ±ã‚’å–å¾—ã—ã€ç™»éŒ²ã™ã‚‹ã€‚ãªã„ã‚‚ã®ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ã™ã‚‹\n",
    "import pandas as pd\n",
    "import myLibStandard as myLib\n",
    "import warnings\n",
    "#warnings.simplefilter('ignore')\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"IR_Bank_Data.csv\",index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":str})\n",
    "dfCode = pd.read_csv(myLib.pathDataSummaryDomesticData + \"CodeData.csv\",usecols=[\"ã‚³ãƒ¼ãƒ‰\", \"éŠ˜æŸ„å\", \"å¸‚å ´ãƒ»å•†å“åŒºåˆ†\", \"33æ¥­ç¨®åŒºåˆ†\"],\\\n",
    "index_col=[\"ã‚³ãƒ¼ãƒ‰\"], dtype=str)\n",
    "dfAll[\"éŠ˜æŸ„\"] = None\n",
    "dfAll[\"å¸‚å ´\"] = None\n",
    "dfAll[\"æ¥­ç¨®\"] = None\n",
    "dfAll = dfAll.sort_index()\n",
    "dlCode = dfAll.groupby(level='ã‚³ãƒ¼ãƒ‰').size().index.tolist()\n",
    "dlDelCode = []\n",
    "for code in dlCode:\n",
    "    try:\n",
    "        dfAll.loc[(code,), \"éŠ˜æŸ„\"] = dfCode.loc[code, \"éŠ˜æŸ„å\"]\n",
    "        dfAll.loc[(code,), \"å¸‚å ´\"] = dfCode.loc[code, \"å¸‚å ´ãƒ»å•†å“åŒºåˆ†\"]\n",
    "        dfAll.loc[(code,), \"æ¥­ç¨®\"] = dfCode.loc[code, \"33æ¥­ç¨®åŒºåˆ†\"]\n",
    "    except KeyError:\n",
    "        dlDelCode.append(code)\n",
    "\n",
    "dfAll = dfAll.reset_index(\"å¹´åº¦\")\n",
    "for dl in dlDelCode:\n",
    "    dfAll = dfAll.drop(dl,axis=0)\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DataPreprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### éŠ˜æŸ„ã®ãƒ†ãƒ¼ãƒæƒ…å ±ã‚’å–å¾—ã—ç™»éŒ²ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ãƒ¼ãƒæƒ…å ±ã‚’CSVã‹ã‚‰ã¨ã£ã¦ãã¦ãƒ†ãƒ¼ãƒãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹\n",
    "import pandas as pd\n",
    "import myLibStandard as myLib\n",
    "import warnings\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"DataPreprocessing.csv\",index_col=\"ã‚³ãƒ¼ãƒ‰\", dtype={\"ã‚³ãƒ¼ãƒ‰\":str})\n",
    "\n",
    "def get_ThemeList(filname):\n",
    "    df = pd.read_csv(myLib.pathDataDomesticData + filname, index_col=\"ã‚³ãƒ¼ãƒ‰\", dtype={\"ã‚³ãƒ¼ãƒ‰\":str})\n",
    "    return df\n",
    "\n",
    "dlname = [\"DOEæ¡ç”¨éŠ˜æŸ„\", \"å††é«˜ãƒ¡ãƒªãƒƒãƒˆ\", \"æ¸›é…ãªã—\", \"è¦ªå­ä¸Šå ´\", \"ç´¯é€²é…å½“\"]\n",
    "for name in dlname:\n",
    "    dfTheme = get_ThemeList(name + \".csv\")\n",
    "    for code in dfTheme.index:\n",
    "        if code in dfAll.index:\n",
    "            dfAll.loc[code, name] = 1\n",
    "\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DataPreprocessing.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†æå€¤ã®è¨ˆç®—ã€è¿½åŠ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ãƒ‡ãƒ¼ã‚¿ã®ç™»éŒ²ï¼ˆPER, PBR, MIXä¿‚æ•°, ç°¡æ˜“ç†è«–æ ªä¾¡, å®‰å…¨åŸŸ, æ™‚ä¾¡ç·é¡, é…å½“åˆ©å›ã‚Š, è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPERã‚’ç™»éŒ²ã™ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER, PBR, ç°¡æ˜“ç†è«–æ ªä¾¡, å®‰å…¨åŸŸ, é…å½“åˆ©å›ã‚Š, MIXä¿‚æ•°, æ™‚ä¾¡ç·é¡, ã‚’ç™»éŒ²ã™ã‚‹ (â€»é•·æ™‚é–“)\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import calendar\n",
    "from yahooquery import Ticker\n",
    "import myLibStandard as myLib\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "#dfSymbol = pd.read_csv(myLib.pathDataSummaryDomesticData + \"symbol.csv\", dtype={\"symbol\":str})\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"DataPreprocessing.csv\",index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "#dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\",index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":str})\n",
    "\n",
    "dfAll[\"PER\"] = math.nan\n",
    "dfAll[\"PBR\"] = math.nan\n",
    "dfAll[\"ç°¡æ˜“ç†è«–æ ªä¾¡\"] = math.nan\n",
    "dfAll[\"å®‰å…¨åŸŸ\"] = math.nan\n",
    "dfAll[\"é…å½“åˆ©å›ã‚Š\"] = math.nan\n",
    "dfAll[\"MIXä¿‚æ•°\"] = math.nan\n",
    "dfAll[\"æ™‚ä¾¡ç·é¡\"] = math.nan\n",
    "dfAll[\"æ ªä¾¡\"] = math.nan\n",
    "dfAll[\"å–¶æ¥­åˆ©ç›Šç‡\"] = math.nan\n",
    "dfAll[\"çµŒå¸¸åˆ©ç›Šç‡\"] = math.nan\n",
    "dfAll[\"ç´”åˆ©ç›Šç‡\"] = math.nan\n",
    "dfAll[\"é…å½“å°†æ¥æœŸå¾…\"] = round(dfAll[\"ROE\"] - dfAll[\"ç´”è³‡ç”£é…å½“ç‡\"] ,1)\n",
    "\n",
    "def get_StockValue(symbol):\n",
    "    yqData = Ticker(symbol+\".T\")\n",
    "    stockValue = yqData.history(start=\"2010-03-01\")\n",
    "    stockValue = stockValue.reset_index(level=[\"date\",'symbol'])\n",
    "    stockValue[\"date\"] = stockValue[\"date\"].apply(lambda x: pd.to_datetime(x).tz_localize(None))\n",
    "    stockValue[\"date\"] = pd.to_datetime(stockValue[\"date\"])\n",
    "    stockValue = stockValue.set_index(\"date\")\n",
    "    stockValue = stockValue[\"close\"].resample(\"D\").ffill()\n",
    "    return stockValue\n",
    "    #return pd.Series()\n",
    "\n",
    "dfAll = dfAll.sort_index()\n",
    "dlCode = dfAll.groupby(level='ã‚³ãƒ¼ãƒ‰').size().index.tolist()\n",
    "i=0\n",
    "for code in dlCode:\n",
    "    if (i>=0) and (i<5000):\n",
    "        print(code)\n",
    "        dsStock = get_StockValue(str(code))\n",
    "        for date in dfAll.loc[(code,)].index:\n",
    "            dtDate = datetime.datetime.strptime(date, '%Y/%m')\n",
    "            dtDate = datetime.date(dtDate.year, dtDate.month, calendar.monthrange(dtDate.year, dtDate.month)[1])\n",
    "            try:\n",
    "                stockValue = dsStock.loc[dtDate.strftime(\"%Y-%m-%d\")]\n",
    "            except KeyError:\n",
    "                stockValue = math.nan\n",
    "            eps = float(dfAll.loc[(code, date), \"EPS\"])\n",
    "            bps = float(dfAll.loc[(code, date), \"BPS\"])\n",
    "            if eps < 0:\n",
    "                dfAll.loc[(code, date), \"PER\"] = math.nan\n",
    "            else:\n",
    "                dfAll.loc[(code, date), \"PER\"] = round(stockValue / eps, 2)\n",
    "            dfAll.loc[(code, date), \"ç°¡æ˜“ç†è«–æ ªä¾¡\"] = round(bps + eps * 10,2)\n",
    "            dfAll.loc[(code, date), \"PBR\"] = round(stockValue / bps, 2)\n",
    "            dfAll.loc[(code, date), \"é…å½“åˆ©å›ã‚Š\"] = round(float(dfAll.loc[(code, date), \"ä¸€æ ªé…å½“\"] / stockValue *100),2)\n",
    "            dfAll.loc[(code, date), \"å®‰å…¨åŸŸ\"] = round(dfAll.loc[(code, date), \"ç°¡æ˜“ç†è«–æ ªä¾¡\"] / stockValue, 2)\n",
    "            dfAll.loc[(code, date), \"MIXä¿‚æ•°\"] = dfAll.loc[(code, date), \"PER\"] * dfAll.loc[(code, date), \"PBR\"]\n",
    "            dfAll.loc[(code, date), \"æ™‚ä¾¡ç·é¡\"] = dfAll.loc[(code, date), \"PER\"] * dfAll.loc[(code, date), \"ç´”åˆ©ç›Š\"]\n",
    "            dfAll.loc[(code, date), \"æ ªä¾¡\"] = stockValue\n",
    "            dfAll.loc[(code, date), \"å–¶æ¥­åˆ©ç›Šç‡\"] = round(dfAll.loc[(code, date), \"å–¶æ¥­åˆ©ç›Š\"] / dfAll.loc[(code, date), \"å£²ä¸Šé«˜\"] * 100 ,2)\n",
    "            dfAll.loc[(code, date), \"çµŒå¸¸åˆ©ç›Šç‡\"] = round(dfAll.loc[(code, date), \"çµŒå¸¸åˆ©ç›Š\"] / dfAll.loc[(code, date), \"å£²ä¸Šé«˜\"] * 100 ,2)\n",
    "            dfAll.loc[(code, date), \"ç´”åˆ©ç›Šç‡\"] = round(dfAll.loc[(code, date), \"ç´”åˆ©ç›Š\"] / dfAll.loc[(code, date), \"å£²ä¸Šé«˜\"] * 100 ,2)\n",
    "    i+=1\n",
    "    if i%10==0:\n",
    "        print(code,i)\n",
    "        dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\")\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPERã‚’ç™»éŒ²ã™ã‚‹ (â€»é•·æ™‚é–“)\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import calendar\n",
    "from yahooquery import Ticker\n",
    "import yfinance as yf\n",
    "import myLibStandard as myLib\n",
    "import warnings\n",
    "#warnings.simplefilter('ignore', FutureWarning)\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "#dfSymbol = pd.read_csv(myLib.pathDataSummaryDomesticData + \"symbol.csv\", dtype={\"symbol\":\"str\"})\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\",index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":\"str\"})\n",
    "dfAll[\"è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPER\"] = math.nan\n",
    "\n",
    "dfAll = dfAll.sort_index()\n",
    "dlCode = dfAll.groupby(level='ã‚³ãƒ¼ãƒ‰').size().index.tolist()\n",
    "#display(dlCode)\n",
    "i=0\n",
    "for code in dlCode:\n",
    "    if (i>=0) & (i<=5000):\n",
    "        print(code)\n",
    "        try:\n",
    "            balance_sheet = Ticker(code+\".T\").balance_sheet().reset_index().set_index(\"asOfDate\")\n",
    "        except AttributeError:\n",
    "            balance_sheet = Ticker(code+\".T\").balance_sheet()\n",
    "        #display(balance_sheet)\n",
    "        if type(balance_sheet) != str:\n",
    "            for date in balance_sheet.index:\n",
    "                dtDate = datetime.date(date.year,date.month,1)\n",
    "                strDate = datetime.date.strftime(dtDate, \"%Y/%m\")\n",
    "                try:\n",
    "                    CurrentAssets = balance_sheet.loc[date, \"CurrentAssets\"]\n",
    "                except KeyError:\n",
    "                    CurrentAssets = math.nan\n",
    "                try:\n",
    "                    NetDebt = balance_sheet.loc[date, \"NetDebt\"]\n",
    "                except KeyError:\n",
    "                    NetDebt = balance_sheet.loc[date, \"TotalAssets\"] - balance_sheet.loc[date, \"StockholdersEquity\"]\n",
    "                try:\n",
    "                    AvailableForSaleSecurities = balance_sheet.loc[date, \"AvailableForSaleSecurities\"]\n",
    "                except KeyError:\n",
    "                    AvailableForSaleSecurities = math.nan\n",
    "                netCash = CurrentAssets + NetDebt * 0.7 -AvailableForSaleSecurities\n",
    "                #print(netCash)\n",
    "                #print(date)\n",
    "                #display(dfAll.loc[(code, )].index)\n",
    "                try:\n",
    "                    dfAll.loc[(code, strDate), \"è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPER\"] = round(\\\n",
    "                    (dfAll.loc[(code, strDate), \"æ™‚ä¾¡ç·é¡\"] - netCash) / dfAll.loc[(code, strDate), \"ç´”åˆ©ç›Š\"], 2)\n",
    "                except KeyError:\n",
    "                    dfAll.loc[(code, strDate),:] = math.nan\n",
    "                    dfAll.loc[(code, strDate), \"è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPER\"] = round(\\\n",
    "                    (dfAll.loc[(code, strDate), \"æ™‚ä¾¡ç·é¡\"] - netCash) / dfAll.loc[(code, strDate), \"ç´”åˆ©ç›Š\"], 2)\n",
    "                #print(code, strDate, dfAll.loc[(code, strDate), \"è³‡ç”£ä¾¡å€¤ã‚’å«ã‚ãŸPER\"])\n",
    "    i+=1\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "        dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\")\n",
    "        #break\n",
    "dfAll.to_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ€æ–°æ—¥ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€æ–°æ—¥ä»˜ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã™ã‚‹ï¼ˆâ€»ä¸­æ™‚é–“ï¼‰\n",
    "import pandas as pd\n",
    "import myLibStandard as myLib\n",
    "myLib.set_DisplayOption()\n",
    "\n",
    "dfAll = pd.read_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTable.csv\",parse_dates=[1],index_col=[\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"], dtype={\"ã‚³ãƒ¼ãƒ‰\":str})\n",
    "dfAll = dfAll.sort_index()\n",
    "dlCode = dfAll.groupby(level='ã‚³ãƒ¼ãƒ‰').size().index.tolist()\n",
    "dfMaxDate = pd.DataFrame()\n",
    "for code in dlCode:\n",
    "    maxDate = dfAll.loc[(code,)].index.max()\n",
    "    #display(dfAll.loc[(code,maxDate)])\n",
    "    #print(code)\n",
    "    if maxDate.year >= 2024:\n",
    "        ds = dfAll.loc[(code,maxDate)].to_frame().T\n",
    "        dfMaxDate = pd.concat([dfMaxDate,ds],axis=0)\n",
    "dfMaxDate.index.names = [\"ã‚³ãƒ¼ãƒ‰\", \"å¹´åº¦\"]\n",
    "dfMaxDate.to_csv(myLib.pathDataSummaryDomesticData + \"DomesticDataTableMaxDate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
